{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37427155-09ae-4c9c-ac73-7e203e9050df",
   "metadata": {},
   "source": [
    "# Frequency script\n",
    "This notebook is used to extract the frequency for a list of search terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d43fbc-6e7e-4aac-89a8-6ff8d3c8c19a",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0329817a-adae-4e1d-b967-3600e0df336f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: en_BBCNews\n",
      "Processing file 532/4744 (11.21%) - “90 killed and 300 injured” in israeli strike on gaza “humanitarian area”  bbc news_output.json in folder: en_BBCNewsewsen_BBCNewsws\n",
      "Finished processing all files in folder: en_BBCNews, results saved to killed_data\\en_BBCNews_killed-murdered.csv\n",
      "Processing folder: en_CNN\n",
      "Processing file 838/4744 (17.66%) - ‘you decided to still drop a bomb’ wolf presses idf spokesman on israeli airstrike on refugee camp_output.json in folder: en_CNNNN\n",
      "Finished processing all files in folder: en_CNN, results saved to killed_data\\en_CNN_killed-murdered.csv\n",
      "Processing folder: en_DW\n",
      "Processing file 1056/4744 (22.26%) - german fm hamas holding entire gaza population hostage  dw news⁣_output.json in folder: en_DWolder: en_DWolder: en_DWW_DW_DWWWKeyError: 'Token'. The column 'Token' does not exist in the DataFrame for File data/en_DW/treetagger_output/german fm hamas holding entire gaza population hostage  dw news⁣_output.json\n",
      "Processing file 1308/4744 (27.57%) - israeli military says ground troops launched 'localized raids' into gaza  dw news⁣_output.json in folder: en_DW en_DWlder: en_DWKeyError: 'Token'. The column 'Token' does not exist in the DataFrame for File data/en_DW/treetagger_output/israeli military says ground troops launched 'localized raids' into gaza  dw news⁣_output.json\n",
      "Processing file 1641/4744 (34.59%) - will israel negotiate with hamas for the release of hostages  dw news⁣_output.json in folder: en_DWn in folder: en_DW: en_DWDW_DWKeyError: 'Token'. The column 'Token' does not exist in the DataFrame for File data/en_DW/treetagger_output/will israel negotiate with hamas for the release of hostages  dw news⁣_output.json\n",
      "Processing file 1656/4744 (34.91%) - ‘it’s time for this war to end’_output.json in folder: en_DWgaza  dw news_output.json in folder: en_DW: en_DWn folder: en_DW\n",
      "Finished processing all files in folder: en_DW, results saved to killed_data\\en_DW_killed-murdered.csv\n",
      "Processing folder: en_AJ\n",
      "Processing file 4744/4744 (100.00%) - “who is the superpower the us or israel” the absurdity of airdrops in gaza  the listening post_output.json in folder: en_AJn_AJJ\n",
      "Finished processing all files in folder: en_AJ, results saved to killed_data\\en_AJ_killed-murdered.csv\n",
      "\n",
      "Processing complete for all folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Path to the CSV file containing search terms\n",
    "input_csv_path = \"killed-murdered_input.csv\"\n",
    "file_name_addition = input_csv_path.split(\"_\")[0]\n",
    "\n",
    "# Read the input CSV file\n",
    "search_terms_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Strip trailing (and leading) whitespaces from all string columns\n",
    "search_terms_df = search_terms_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# Define the folders you want to process\n",
    "folder_paths = [\n",
    "    \"data/en_BBCNews/treetagger_output/\",\n",
    "    \"data/en_CNN/treetagger_output/\",\n",
    "    \"data/en_DW/treetagger_output/\",\n",
    "    \"data/en_AJ/treetagger_output/\"\n",
    "]\n",
    "\n",
    "# Create the 'frequency_data' directory if it doesn't exist\n",
    "output_directory = \"killed_data\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "def search_file_ci(search_term, filedf):\n",
    "    # Initialize row_indices to None\n",
    "    row_indices = None\n",
    "\n",
    "    if pd.isna(search_term['Token']):\n",
    "        pass  # Do nothing if Token is NaN\n",
    "    else:\n",
    "        token = search_term['Token'].lower().strip()\n",
    "        match = filedf.apply(lambda col: col.str.lower().str.strip().isin([token]) if col.dtype == 'object' else col.isin([token]))\n",
    "        positions = match.stack()[match.stack()]\n",
    "        row_indices = positions.index.get_level_values(0)\n",
    "\n",
    "    if pd.isna(search_term['Tag']):\n",
    "        pass\n",
    "    else:\n",
    "        tag = search_term['Tag'].lower().strip()\n",
    "        if row_indices is not None:\n",
    "            tag_match = filedf.loc[row_indices, 'Tag'].str.lower().str.strip() == tag\n",
    "            row_indices = row_indices[tag_match]\n",
    "        else:\n",
    "            match = filedf['Tag'].str.lower().str.strip().isin([tag])\n",
    "            row_indices = filedf.index[match]\n",
    "\n",
    "    if pd.isna(search_term['Lemma']):\n",
    "        pass\n",
    "    else:\n",
    "        lemma = search_term['Lemma'].lower().strip()\n",
    "        if row_indices is not None:\n",
    "            lemma_match = filedf.loc[row_indices, 'Lemma'].str.lower().str.strip() == lemma\n",
    "            row_indices = row_indices[lemma_match]\n",
    "        else:\n",
    "            match = filedf['Lemma'].str.lower().str.strip().isin([lemma])\n",
    "            row_indices = filedf.index[match]\n",
    "\n",
    "    return row_indices if row_indices is not None else pd.Index([])\n",
    "\n",
    "def search_term_iterate(search_terms_df, df, file_result):\n",
    "    for _, row in search_terms_df.iterrows():\n",
    "        search_term = {\n",
    "            'Token': row['Token'],\n",
    "            'Tag': row['Tag'],\n",
    "            'Lemma': row['Lemma']\n",
    "        }\n",
    "        \n",
    "        term_column_name = \".\".join(str(value) if pd.notna(value) else \"\" for value in search_term.values())\n",
    "        \n",
    "        preceding_terms = 'TRUE'\n",
    "        preceding_term1 = {'Token': pd.NA, 'Tag': \"VBD\", 'Lemma': pd.NA}\n",
    "        preceding_term2 = {'Token': pd.NA, 'Tag': \"VBN\", 'Lemma': pd.NA}\n",
    "\n",
    "        if preceding_terms == 'TRUE':\n",
    "            indices_single_term = search_file_ci(search_term, df)\n",
    "            indices_preceding_term1 = search_file_ci(preceding_term1, df)\n",
    "            indices_preceding_term2 = search_file_ci(preceding_term2, df)\n",
    "\n",
    "            indices_adjusted_preceding_term1 = [idx + 1 for idx in indices_preceding_term1]\n",
    "            indices_adjusted_preceding_term2 = [idx + 1 for idx in indices_preceding_term2]\n",
    "\n",
    "            common_indices1 = set(indices_single_term).intersection(indices_adjusted_preceding_term1)\n",
    "            common_indices2 = set(indices_single_term).intersection(indices_adjusted_preceding_term2)\n",
    "            common_indices = common_indices1.union(common_indices2)\n",
    "\n",
    "            file_result[term_column_name] = len(common_indices)\n",
    "            \n",
    "            if common_indices:\n",
    "                term_subject = term_column_name + \"subject\"\n",
    "                token1_indices = [idx - 2 for idx in common_indices1 if (idx - 2) in df.index]\n",
    "                token2_indices = [idx - 3 for idx in common_indices2 if (idx - 3) in df.index]\n",
    "\n",
    "                token1 = df.loc[token1_indices, 'Token'].tolist() if token1_indices else [\"\"]\n",
    "                token2 = df.loc[token2_indices, 'Token'].tolist() if token2_indices else [\"\"]\n",
    "\n",
    "                file_result[term_subject] = \" \".join(token1 + token2).strip()\n",
    "\n",
    "    return file_result\n",
    "\n",
    "total_files = sum([len([name for name in os.listdir(folder) if name.endswith(\".json\")]) for folder in folder_paths])\n",
    "processed_files = 0\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "    middle_folder_name = folder_path.split('/')[1]\n",
    "    print(f\"Processing folder: {middle_folder_name}\")\n",
    "    \n",
    "    folder_results = []\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            processed_files += 1\n",
    "            progress_percentage = (processed_files / total_files) * 100\n",
    "            print(f\"\\rProcessing file {processed_files}/{total_files} ({progress_percentage:.2f}%) - {file_name} in folder: {middle_folder_name}\", end='', flush=True)\n",
    "            \n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            \n",
    "            video_id = data['video_id']\n",
    "            publish_date = data['publish_date']\n",
    "            video_title = data['video_title']\n",
    "              \n",
    "            treetagger_output = data['treetagger_output']\n",
    "              \n",
    "            content_df = pd.DataFrame(treetagger_output)\n",
    "            try:\n",
    "                total_word_count = content_df[\"Token\"].fillna('').str.split().str.len().sum()\n",
    "                total_word_count = int(total_word_count)\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError: {e}. The column 'Token' does not exist in the DataFrame for File {file_path}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {e}\")\n",
    "                continue\n",
    "\n",
    "            file_result = {\n",
    "                \"file_name\": file_name,\n",
    "                \"video_id\": video_id,\n",
    "                \"publish_date\": publish_date,\n",
    "                \"total_word_count\": total_word_count\n",
    "            }\n",
    "            file_result = search_term_iterate(search_terms_df, content_df, file_result)\n",
    "            folder_results.append(file_result)\n",
    "\n",
    "    folder_df = pd.DataFrame(folder_results)\n",
    "\n",
    "    output_csv_path = os.path.join(output_directory, f\"{middle_folder_name}_{file_name_addition}.csv\")\n",
    "    folder_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"\\nFinished processing all files in folder: {middle_folder_name}, results saved to {output_csv_path}\")\n",
    "\n",
    "print(\"\\nProcessing complete for all folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66ebd2-edb3-427b-b008-9f7878e321e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
