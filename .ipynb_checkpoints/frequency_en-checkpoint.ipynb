{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37427155-09ae-4c9c-ac73-7e203e9050df",
   "metadata": {},
   "source": [
    "# Frequency script\n",
    "This notebook is used to extract the frequency for a list of search terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d43fbc-6e7e-4aac-89a8-6ff8d3c8c19a",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47718b-20a5-4e2b-b50e-2693319f0e60",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae158449-2e3d-425e-9ff7-977d925b5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925aedcd-603e-4949-aebf-1f8a1e075b09",
   "metadata": {},
   "source": [
    "### define search terms\n",
    "The search terms for the frequency analysis are located in a search mask.\n",
    "This is currently located in a csv file, with the following structure:\n",
    "| Token| Tag|Lemma\n",
    "\n",
    "For searching for a specific token (e.g. \"Palestinians\") the search term needs to be placed in the Token column, for a specific Tag (e.g. \"VVN\") in the Tag column and to look for a Lemma in the Lemma column.\n",
    "Currently having things in more than one column, doesn't work.\n",
    "Several words per cell does not properly seem implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90192334-142a-4b81-9ea3-b4f6d4c81751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file containing search terms\n",
    "input_csv_path = \"freqency_en_input.csv\"\n",
    "\n",
    "# Read the input CSV file\n",
    "search_terms_df = pd.read_csv(input_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf306310-cc88-4de1-b8f7-5e2abbd865ce",
   "metadata": {},
   "source": [
    "### define the folders\n",
    "the folders in folder_paths will be searched for the search terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f8a997-b32a-4889-ba10-f5824ceb7302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folders you want to process\n",
    "folder_paths = [\n",
    "    \"data/en_BBCNews/treetagger_output/\",\n",
    "    \"data/en_CNN/treetagger_output/\",\n",
    "    \"data/en_DW/treetagger_output/\",\n",
    "    \"data/en_AJ/treetagger_output/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5172683-49ce-4cf1-abb4-30393ca42679",
   "metadata": {},
   "source": [
    "### create output folder\n",
    "here a .csv file with the frequency results will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2693d63d-7a8e-4124-a901-d7a580e28620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'frequency_data' directory if it doesn't exist\n",
    "output_directory = \"frequency_data\"\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2b1a3-cb0e-4ae8-9262-7fd29bc17b4b",
   "metadata": {},
   "source": [
    "### Count total files\n",
    "For keeping track how far along the script is, we track processed_files and will compare them to total_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d12bc8-f015-4573-9682-07a550fc6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of files to be processed\n",
    "total_files = sum([len([name for name in os.listdir(folder) if name.endswith(\".txt\")]) for folder in folder_paths])\n",
    "processed_files = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea3c1f-00b8-45ed-b68b-99b4477448a9",
   "metadata": {},
   "source": [
    "### Initiate cleaning function\n",
    "In some cases the txt structure contains an absurd combination of different quotation marks which will break the script. If that happens and onyl then, we preprocess the offending file to deal with the quotation marks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836e79e-bec1-4fee-945d-f93f17868635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the file to handle unusual lines\n",
    "def preprocess_file(file_path):\n",
    "    cleaned_lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        for line in file:\n",
    "            # Check for and handle unusual characters or formatting\n",
    "            if line.count('\"') % 2 == 0:  # Ensure the line has an even number of quotes\n",
    "                cleaned_lines.append(line)\n",
    "            elif line.strip():  # If the line is not empty, clean it up\n",
    "                cleaned_line = line.replace('“', '\"').replace('”', '\"')  # Replace unusual quotes with standard quotes\n",
    "                cleaned_line = cleaned_line.replace('``', '\"')  # Replace backticks with quotes\n",
    "                cleaned_lines.append(cleaned_line)\n",
    "    \n",
    "    # Write the cleaned lines to a temporary file\n",
    "    cleaned_file_path = file_path.replace(\".txt\", \"_cleaned.txt\")\n",
    "    with open(cleaned_file_path, 'w', encoding='utf-8') as cleaned_file:\n",
    "        cleaned_file.writelines(cleaned_lines)\n",
    "    \n",
    "    return cleaned_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959cdd9a-4433-4116-8db1-10bc22ae351e",
   "metadata": {},
   "source": [
    "### initiate function for metadata and content\n",
    "the metadata is located at the beginning of the txt files while the actually content is between <doc> and </doc> tags.\n",
    "Currently no words in the titles of videos are analyzed.\n",
    "There could be a switch for that (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d5f57-bef3-4f28-80da-f129d6a1f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract metadata and content between <doc> and </doc> tags\n",
    "def extract_metadata_and_content(df):\n",
    "    video_id = None\n",
    "    publish_date = None\n",
    "    content_start = None\n",
    "    content_end = None\n",
    "\n",
    "    # Fill NaN values with an empty string to prevent issues with `in` checks\n",
    "    df = df.fillna('')\n",
    "\n",
    "    # Iterate through the DataFrame to find metadata and content boundaries\n",
    "    for index, row in df.iterrows():\n",
    "        row_content = row.iloc[0]\n",
    "        if '<video_id>' in row_content:\n",
    "            video_id = re.search(r'<video_id>(.*?)</video_id>', row_content).group(1)\n",
    "        if '<publish_date>' in row_content:\n",
    "            publish_date = re.search(r'<publish_date>(.*?)</publish_date>', row_content).group(1)\n",
    "        if '<doc>' in row_content:\n",
    "            content_start = index + 1  # Start after <doc> tag\n",
    "        if '</doc>' in row_content:\n",
    "            content_end = index  # End before </doc> tag\n",
    "            break  # Exit loop once end tag is found\n",
    "\n",
    "    return video_id, publish_date, content_start, content_end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901a35a-1745-4760-9458-05c0dbf0799a",
   "metadata": {},
   "source": [
    "## Main loop\n",
    "Here we iterate through folders, and text files to count the frequency of the search terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0329817a-adae-4e1d-b967-3600e0df336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: en_BBCNews\n",
      "Processing file 533/4743 (11.24%) - “90 killed and 300 injured” in israeli strike on gaza “humanitarian area”  bbc news_treetagger_output.txt in folder: en_BBCNewsewsen_BBCNewsws\n",
      "Finished processing all files in folder: en_BBCNews, results saved to frequency_data\\en_BBCNews_frequency.csv\n",
      "Processing folder: en_CNN\n",
      "Processing file 839/4743 (17.69%) - ‘you decided to still drop a bomb’ wolf presses idf spokesman on israeli airstrike on refugee camp_treetagger_output.txt in folder: en_CNNNN\n",
      "Finished processing all files in folder: en_CNN, results saved to frequency_data\\en_CNN_frequency.csv\n",
      "Processing folder: en_DW\n",
      "Processing file 1654/4743 (34.87%) - ‘it’s time for this war to end’_treetagger_output.txt in folder: en_DWews_treetagger_output.txt in folder: en_DW: en_DWn folder: en_DWDW_DW\n",
      "Finished processing all files in folder: en_DW, results saved to frequency_data\\en_DW_frequency.csv\n",
      "Processing folder: en_AJ\n",
      "Processing file 4403/4743 (92.83%) - war on gaza challenges for palestinians with disabilities_treetagger_output.txt in folder: en_AJr: en_AJn folder: en_AJn_AJJJ: en_AJAJAJJJJ\n",
      "ParserError with default engine for file: war on gaza challenges for palestinians with disabilities_treetagger_output.txt. Preprocessing and retrying with engine='python'.\n",
      "Processing file 4404/4743 (92.85%) - war on gaza challenges for palestinians with disabilities_treetagger_output_cleaned.txt in folder: en_AJ\n",
      "ParserError with default engine for file: war on gaza challenges for palestinians with disabilities_treetagger_output_cleaned.txt. Preprocessing and retrying with engine='python'.\n",
      "Processing file 4743/4743 (100.00%) - “who is the superpower the us or israel” the absurdity of airdrops in gaza  the listening post_treetagger_output.txt in folder: en_AJn_AJ\n",
      "Finished processing all files in folder: en_AJ, results saved to frequency_data\\en_AJ_frequency.csv\n",
      "\n",
      "Processing complete for all folders.\n"
     ]
    }
   ],
   "source": [
    "# Loop through all folder paths\n",
    "for folder_path in folder_paths:\n",
    "    # Extract the middle section of the folder path\n",
    "    middle_folder_name = folder_path.split('/')[1]  # Adjust based on your folder structure\n",
    "    print(f\"Processing folder: {middle_folder_name}\")\n",
    "    \n",
    "    # Create a list to store data for the current folder\n",
    "    folder_results = []\n",
    "    \n",
    "    # Loop through all files in the current folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file is a .txt file\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            processed_files += 1\n",
    "            progress_percentage = (processed_files / total_files) * 100\n",
    "            print(f\"\\rProcessing file {processed_files}/{total_files} ({progress_percentage:.2f}%) - {file_name} in folder: {middle_folder_name}\", end='', flush=True)\n",
    "            \n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Attempt to read the file using the default 'c' engine first\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, delimiter=\"\\t\", header=None, names=[\"col1\", \"col2\", \"col3\"], dtype=str, on_bad_lines='skip')\n",
    "            except pd.errors.ParserError:\n",
    "                print(f\"\\nParserError with default engine for file: {file_name}. Preprocessing and retrying with engine='python'.\")\n",
    "                # Preprocess the file to handle problematic lines\n",
    "                cleaned_file_path = preprocess_file(file_path)\n",
    "                # Retry with the cleaned file using the Python engine\n",
    "                df = pd.read_csv(cleaned_file_path, delimiter=\"\\t\", header=None, names=[\"col1\", \"col2\", \"col3\"], dtype=str, engine='python', on_bad_lines='skip')\n",
    "\n",
    "            \n",
    "            # Extract metadata and content range\n",
    "            video_id, publish_date, content_start, content_end = extract_metadata_and_content(df)\n",
    "            \n",
    "            # Skip files that don't contain the expected tags\n",
    "            if content_start is None or content_end is None:\n",
    "                continue\n",
    "            \n",
    "            # Extract the relevant content\n",
    "            content_df = df.iloc[content_start:content_end]\n",
    "            \n",
    "            # Calculate total word count\n",
    "            total_word_count = content_df[\"col3\"].fillna('').str.split().str.len().sum()\n",
    "            total_word_count = int(total_word_count)  # Ensure it's an integer\n",
    "\n",
    "            # Initialize a dictionary to store term counts for this file\n",
    "            file_result = {\n",
    "                \"file_name\": file_name,\n",
    "                \"video_id\": video_id,\n",
    "                \"publish_date\": publish_date,\n",
    "                \"total_word_count\": total_word_count\n",
    "            }\n",
    "            \n",
    "            # Loop through each row in the search terms CSV\n",
    "            for _, row in search_terms_df.iterrows():\n",
    "                # Process columns 'Token', 'Tag', and 'Lemma'\n",
    "                columns_to_search = {\n",
    "                    'Token': row['Token'],\n",
    "                    'Tag': row['Tag'],\n",
    "                    'Lemma': row['Lemma']\n",
    "                }\n",
    "                \n",
    "                for col_name, term_to_search in columns_to_search.items():\n",
    "                    if pd.notna(term_to_search) and term_to_search.strip():\n",
    "                        term_to_search = str(term_to_search).strip()\n",
    "                        column_index = {'Token': 2, 'Tag': 1, 'Lemma': 0}[col_name]  # Corresponding column index in DataFrame\n",
    "                        term_column_name = f\"{col_name}_{term_to_search}\"\n",
    "                        \n",
    "                        # If the term contains two words, search for them as a sequence\n",
    "                        words = term_to_search.split()\n",
    "                        if len(words) == 2:\n",
    "                            first_word, second_word = words\n",
    "                            # Check for the sequence of two words\n",
    "                            count_two_word_occurrences = (\n",
    "                                content_df.iloc[:, column_index].str.contains(fr'\\b{first_word}\\b', case=False, na=False) & \n",
    "                                content_df.iloc[:, column_index + 1].str.contains(fr'\\b{second_word}\\b', case=False, na=False)\n",
    "                            ).sum()\n",
    "                            \n",
    "                            file_result[term_column_name] = count_two_word_occurrences\n",
    "                        else:\n",
    "                            # Handle single-word search term\n",
    "                            file_result[term_column_name] = content_df.iloc[:, column_index].str.contains(fr'\\b{term_to_search}\\b', case=False, na=False).sum()\n",
    "            \n",
    "            # Add the result dictionary to the list for this folder\n",
    "            folder_results.append(file_result)\n",
    "\n",
    "    # Convert the folder results into a DataFrame\n",
    "    folder_df = pd.DataFrame(folder_results)\n",
    "\n",
    "    # Save to a CSV file in the 'frequency_data' directory using the middle folder name\n",
    "    output_csv_path = os.path.join(output_directory, f\"{middle_folder_name}_frequency.csv\")\n",
    "    folder_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"\\nFinished processing all files in folder: {middle_folder_name}, results saved to {output_csv_path}\")\n",
    "\n",
    "print(\"\\nProcessing complete for all folders.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
