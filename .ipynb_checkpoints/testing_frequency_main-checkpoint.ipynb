{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37427155-09ae-4c9c-ac73-7e203e9050df",
   "metadata": {},
   "source": [
    "# Frequency script\n",
    "This notebook is used to extract the frequency for a list of search terms. First some formatting things:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d43fbc-6e7e-4aac-89a8-6ff8d3c8c19a",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47718b-20a5-4e2b-b50e-2693319f0e60",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ae158449-2e3d-425e-9ff7-977d925b5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "90192334-142a-4b81-9ea3-b4f6d4c81751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file containing search terms\n",
    "input_csv_path = \"test_frequency.csv\"\n",
    "# Read the input CSV file\n",
    "search_terms_df = pd.read_csv(input_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "39c0a9d2-a590-43e9-a18a-dfefcf6d35e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Token  Tag                Lemma\n",
      "0     NaN  NaN                 kill\n",
      "1     NaN  NaN                 Gaza\n",
      "2     NaN  NaN                 gaza\n",
      "3  killed  VVN                  NaN\n",
      "4     NaN  NaN       United Nations\n",
      "5     NaN  NaN  Israeli government \n"
     ]
    }
   ],
   "source": [
    "print(search_terms_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "51f8a997-b32a-4889-ba10-f5824ceb7302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file\n",
    "file_path = \"data/en_CNN/treetagger_output/israeli military on high alert for potential imminent attack by iran_output.json\"\n",
    "# Open the JSON file and load the data with utf-8 encoding\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "# Accessing various parts of the JSON data\n",
    "video_id = data['video_id']\n",
    "publish_date = data['publish_date']\n",
    "video_title = data['video_title']\n",
    "\n",
    "# Access the treetagger_output\n",
    "treetagger_output = data['treetagger_output']\n",
    "\n",
    "# Convert the treetagger_output list of dictionaries to a DataFrame\n",
    "df_treetagger = pd.DataFrame(treetagger_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "645748ef-ca58-420d-9382-a13f948cc326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_file_ci(search_term, filedf):\n",
    "    # Initialize row_indices to None\n",
    "    row_indices = None\n",
    "\n",
    "    if pd.isna(search_term['Token']):\n",
    "        pass  # Do nothing if Token is NaN\n",
    "    else:\n",
    "        # Convert search term to lowercase\n",
    "        token = search_term['Token'].lower()\n",
    "        # Create a boolean mask for matches in a case-insensitive manner\n",
    "        match = filedf.apply(lambda col: col.str.lower().isin([token]) if col.dtype == 'object' else col.isin([token]))\n",
    "        # Stack the DataFrame to get the positions where matches occurred\n",
    "        positions = match.stack()[match.stack()]\n",
    "        row_indices = positions.index.get_level_values(0)\n",
    "\n",
    "    if pd.isna(search_term['Tag']):\n",
    "        pass  # Do nothing if Tag is NaN\n",
    "    else:\n",
    "        # Convert search term to lowercase\n",
    "        tag = search_term['Tag'].lower()\n",
    "        if row_indices is not None:\n",
    "            tag_match = filedf.loc[row_indices, 'Tag'].str.lower() == tag\n",
    "            row_indices = row_indices[tag_match]\n",
    "        else:\n",
    "            match = filedf['Tag'].str.lower().isin([tag])\n",
    "            row_indices = filedf.index[match]\n",
    "\n",
    "    if pd.isna(search_term['Lemma']):\n",
    "        pass  # Do nothing if Lemma is NaN\n",
    "    else:\n",
    "        # Convert search term to lowercase\n",
    "        lemma = search_term['Lemma'].lower()\n",
    "        if row_indices is not None:\n",
    "            lemma_match = filedf.loc[row_indices, 'Lemma'].str.lower() == lemma\n",
    "            row_indices = row_indices[lemma_match]\n",
    "        else:\n",
    "            match = filedf['Lemma'].str.lower().isin([lemma])\n",
    "            row_indices = filedf.index[match]\n",
    "\n",
    "    return row_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a4601589-ffd8-425f-a4b1-d9ade643f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_file(search_term, filedf):\n",
    "    if 'row_indices' in locals():\n",
    "          del row_indices\n",
    "    if pd.isna(search_term['Token']):\n",
    "        pass  # Do nothing if Token is NaN\n",
    "    else:\n",
    "        match = filedf.isin([search_term['Token']])\n",
    "        # Stack the DataFrame to get the positions where matches occurred\n",
    "        positions = match.stack()[match.stack()]\n",
    "        row_indices = positions.index.get_level_values(0)\n",
    "    if pd.isna(search_term['Tag']):\n",
    "        pass  # Do nothing if Token is NaN\n",
    "    else:\n",
    "        if 'row_indices' in locals():\n",
    "            tag_match = filedf.loc[row_indices, 'Tag'] == search_term['Tag']\n",
    "            row_indices = row_indices[tag_match]\n",
    "        else:\n",
    "            match = filedf.isin([search_term['Tag']])\n",
    "            # Stack the DataFrame to get the positions where matches occurred\n",
    "            positions = match.stack()[match.stack()]\n",
    "            row_indices = positions.index.get_level_values(0)\n",
    "    if pd.isna(search_term['Lemma']):\n",
    "        pass  # Do nothing if Token is NaN\n",
    "    else:\n",
    "        if 'row_indices' in locals():\n",
    "            lemma_match = filedf.loc[row_indices, 'Lemma'] == search_term['Lemma']\n",
    "            row_indices = row_indices[lemma_match]\n",
    "        else:\n",
    "            match = filedf.isin([search_term['Lemma']])\n",
    "            # Stack the DataFrame to get the positions where matches occurred\n",
    "            positions = match.stack()[match.stack()]\n",
    "            row_indices = positions.index.get_level_values(0)\n",
    "    return(row_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b50282be-39b4-4b23-ac68-e6c960e0efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_two_word_entries(entry):\n",
    "    # Create a copy of the original dictionary for the first entry\n",
    "    first_dict = entry.copy()\n",
    "    second_dict = {k: pd.NA for k in entry}  # Initialize the second dictionary with pd.NA\n",
    "    \n",
    "    for key, value in entry.items():\n",
    "        if isinstance(value, str) and len(value.split()) == 2:  # Check if value is a string and has two words\n",
    "            word1, word2 = value.split()\n",
    "            first_dict[key] = word1  # Replace the two-word entry with the first word\n",
    "            second_dict[key] = word2  # Store the second word in the second dictionary\n",
    "            break  # Stop once a two-word entry is found and split\n",
    "            \n",
    "    return first_dict, second_dict  # Return both dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "33e60727-d982-4fc8-9397-5651e8d32442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_two_terms(search_term):\n",
    "    two_terms = 'FALSE'\n",
    "    for key, value in search_term.items():\n",
    "        if isinstance(value, str) and len(value.split()) == 2: \n",
    "            #print(\"!two terms found\")\n",
    "            two_terms= 'TRUE'\n",
    "    return two_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55de17-92c8-496e-9f2e-4a8c05cfb836",
   "metadata": {},
   "source": [
    "there is currently a bug where for some reason with double terms it gets every index twice. Through the intersection this disappears again though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0329817a-adae-4e1d-b967-3600e0df336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='int64')\n",
      "Index([1429, 1984, 1995], dtype='int64')\n",
      "Index([1429, 1984, 1995], dtype='int64')\n",
      "Index([], dtype='int64')\n",
      "indices_single_term:  Index([928, 946, 1390, 1473], dtype='int64')\n",
      "indices_following_term:  Index([], dtype='int64')\n",
      "Two term indices: 0\n",
      "indices_single_term:  Index([86, 271, 438, 1345], dtype='int64')\n",
      "indices_following_term:  Index([272, 439, 2266], dtype='int64')\n",
      "Two term indices: 2\n",
      "{'file_name': 'random_name', '..kill': 0, '..Gaza': 3, '..gaza': 3, 'killed.VVN.': 0, '..United Nations': 0, '..Israeli government ': 2}\n"
     ]
    }
   ],
   "source": [
    "# Loop through each row in the search terms CSV\n",
    "file_result = {\n",
    "    \"file_name\": \"random_name\",\n",
    "            }\n",
    "for _, row in search_terms_df.iterrows():\n",
    "    # Process columns 'Token', 'Tag', and 'Lemma'\n",
    "    search_term = {\n",
    "    'Token': row['Token'],\n",
    "    'Tag': row['Tag'],\n",
    "    'Lemma': row['Lemma']\n",
    "    }\n",
    "    print(df_treetagger)\n",
    "    #Here is defined what the column of the search result will look like\n",
    "    term_column_name = \".\".join(str(value) if pd.notna(value) else \"\" for value in search_term.values())\n",
    "    #print(search_term)\n",
    "    two_terms= check_two_terms(search_term)\n",
    "    #check for two word terms:\n",
    "    if two_terms=='FALSE':\n",
    "        print(search_file_ci(search_term,df_treetagger))\n",
    "        file_result[term_column_name] = len(search_file_ci(search_term,df_treetagger))\n",
    "    if two_terms=='TRUE':\n",
    "        # Perform the split and assign to separate variables\n",
    "        single_term, following_term = split_two_word_entries(search_term)\n",
    "        indices_single_term = search_file_ci(single_term,df)\n",
    "        indices_following_term = search_file_ci(following_term,df)\n",
    "        # Keep only the indices that are present in both sets\n",
    "        print(\"indices_single_term: \",indices_single_term)\n",
    "        print(\"indices_following_term: \",indices_following_term)\n",
    "        common_indices = indices_single_term.intersection(indices_following_term-1)\n",
    "        # Output the result\n",
    "        print(\"Two term indices:\", len(common_indices))\n",
    "        file_result[term_column_name] = len(common_indices)\n",
    "\n",
    "print(file_result)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24eada-2ccd-4d26-9764-8788a40d337d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
