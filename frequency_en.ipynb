{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37427155-09ae-4c9c-ac73-7e203e9050df",
   "metadata": {},
   "source": [
    "# Frequency script\n",
    "This notebook is used to extract the frequency for a list of search terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d43fbc-6e7e-4aac-89a8-6ff8d3c8c19a",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47718b-20a5-4e2b-b50e-2693319f0e60",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae158449-2e3d-425e-9ff7-977d925b5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925aedcd-603e-4949-aebf-1f8a1e075b09",
   "metadata": {},
   "source": [
    "### define search terms\n",
    "The search terms for the frequency analysis are located in a search mask.\n",
    "This is currently located in a csv file, with the following structure:\n",
    "| Token | Tag | Lemma |\n",
    "|-------|-----|-------|\n",
    "| sings |     |       |\n",
    "|       | VVN |       |\n",
    "|       |     | sing  |\n",
    "\n",
    "For searching for a specific token (e.g. \"Palestinians\") the search term needs to be placed in the Token column, for a specific Tag (e.g. \"VVN\") in the Tag column and to look for a Lemma in the Lemma column.\n",
    "Currently having things in more than one column, doesn't work.\n",
    "Several words per cell does not properly seem implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90192334-142a-4b81-9ea3-b4f6d4c81751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file containing search terms\n",
    "input_csv_path = \"frequency_en_input.csv\"\n",
    "file_name_addition = input_csv_path.split(\"_\")[0]\n",
    "\n",
    "# Read the input CSV file\n",
    "search_terms_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Strip trailing (and leading) whitespaces from all string columns\n",
    "search_terms_df = search_terms_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# Now search_terms_df has no trailing whitespaces in any of the cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04b528-da9c-4c56-a9cb-6c00908a71ea",
   "metadata": {},
   "source": [
    "file_name_addition is a variable that will be used so the output file reflects the input filename."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf306310-cc88-4de1-b8f7-5e2abbd865ce",
   "metadata": {},
   "source": [
    "### define the folders\n",
    "the folders in folder_paths will be searched for the search terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f8a997-b32a-4889-ba10-f5824ceb7302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folders you want to process\n",
    "folder_paths = [\n",
    "    \"data/en_BBCNews/treetagger_output/\",\n",
    "    \"data/en_CNN/treetagger_output/\",\n",
    "    \"data/en_DW/treetagger_output/\",\n",
    "    \"data/en_AJ/treetagger_output/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5172683-49ce-4cf1-abb4-30393ca42679",
   "metadata": {},
   "source": [
    "### create output folder\n",
    "here a .csv file with the frequency results will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2693d63d-7a8e-4124-a901-d7a580e28620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'frequency_data' directory if it doesn't exist\n",
    "output_directory = \"frequency_data_new\"\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f3609-ea49-46d8-af5c-522a7fc64d52",
   "metadata": {},
   "source": [
    "## Initiate functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8cb5b-e709-442c-8b35-0d744e29e47f",
   "metadata": {},
   "source": [
    "### initiate search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bde4caa-323d-4ba7-973a-085e99e27dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_file(search_term, filedf):\n",
    "    if 'row_indices' in locals():\n",
    "          del row_indices\n",
    "    if pd.isna(search_term['Token']):\n",
    "        pass  # Do nothing if Token is NaN\n",
    "    else:\n",
    "        match = filedf.isin([search_term['Token']])\n",
    "        # Stack the DataFrame to get the positions where matches occurred\n",
    "        positions = match.stack()[match.stack()]\n",
    "        row_indices = positions.index.get_level_values(0)\n",
    "    if pd.isna(search_term['Tag']):\n",
    "        pass  # Do nothing if Token is NaN\n",
    "    else:\n",
    "        if 'row_indices' in locals():\n",
    "            tag_match = filedf.loc[row_indices, 'Tag'] == search_term['Tag']\n",
    "            row_indices = row_indices[tag_match]\n",
    "        else:\n",
    "            match = filedf.isin([search_term['Tag']])\n",
    "            # Stack the DataFrame to get the positions where matches occurred\n",
    "            positions = match.stack()[match.stack()]\n",
    "            row_indices = positions.index.get_level_values(0)\n",
    "    if pd.isna(search_term['Lemma']):\n",
    "        pass  # Do nothing if Token is NaN\n",
    "    else:\n",
    "        if 'row_indices' in locals():\n",
    "            lemma_match = filedf.loc[row_indices, 'Lemma'] == search_term['Lemma']\n",
    "            row_indices = row_indices[lemma_match]\n",
    "        else:\n",
    "            match = filedf.isin([search_term['Lemma']])\n",
    "            # Stack the DataFrame to get the positions where matches occurred\n",
    "            positions = match.stack()[match.stack()]\n",
    "            row_indices = positions.index.get_level_values(0)\n",
    "    return(row_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08edc23e-2297-4c47-8263-c15d3bde344a",
   "metadata": {},
   "source": [
    "Below the same function but case insensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7889ad-3b02-4782-a18c-07fcef27c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_file_ci(search_term, filedf):\n",
    "    # Initialize row_indices to None\n",
    "    row_indices = None\n",
    "\n",
    "    if pd.isna(search_term['Token']):\n",
    "        pass  # Do nothing if Token is NaN\n",
    "    else:\n",
    "        # Convert search term to lowercase\n",
    "        token = search_term['Token'].lower()\n",
    "        # Create a boolean mask for matches in a case-insensitive manner\n",
    "        match = filedf.apply(lambda col: col.str.lower().isin([token]) if col.dtype == 'object' else col.isin([token]))\n",
    "        # Stack the DataFrame to get the positions where matches occurred\n",
    "        positions = match.stack()[match.stack()]\n",
    "        row_indices = positions.index.get_level_values(0)\n",
    "\n",
    "    if pd.isna(search_term['Tag']):\n",
    "        pass  # Do nothing if Tag is NaN\n",
    "    else:\n",
    "        # Convert search term to lowercase\n",
    "        tag = search_term['Tag'].lower()\n",
    "        if row_indices is not None:\n",
    "            tag_match = filedf.loc[row_indices, 'Tag'].str.lower() == tag\n",
    "            row_indices = row_indices[tag_match]\n",
    "        else:\n",
    "            match = filedf['Tag'].str.lower().isin([tag])\n",
    "            row_indices = filedf.index[match]\n",
    "\n",
    "    if pd.isna(search_term['Lemma']):\n",
    "        pass  # Do nothing if Lemma is NaN\n",
    "    else:\n",
    "        # Convert search term to lowercase\n",
    "        lemma = search_term['Lemma'].lower()\n",
    "        if row_indices is not None:\n",
    "            lemma_match = filedf.loc[row_indices, 'Lemma'].str.lower() == lemma\n",
    "            row_indices = row_indices[lemma_match]\n",
    "        else:\n",
    "            match = filedf['Lemma'].str.lower().isin([lemma])\n",
    "            row_indices = filedf.index[match]\n",
    "\n",
    "    return row_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69f0b3-49da-4cc2-9810-098453e30096",
   "metadata": {},
   "source": [
    "### functions to deal with two word terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ca1cf7f-275e-44bf-abc0-e344c3f280bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_two_word_entries(entry):\n",
    "    # Create a copy of the original dictionary for the first entry\n",
    "    first_dict = entry.copy()\n",
    "    second_dict = {k: pd.NA for k in entry}  # Initialize the second dictionary with pd.NA\n",
    "    \n",
    "    for key, value in entry.items():\n",
    "        if isinstance(value, str) and len(value.split()) == 2:  # Check if value is a string and has two words\n",
    "            word1, word2 = value.split()\n",
    "            first_dict[key] = word1  # Replace the two-word entry with the first word\n",
    "            second_dict[key] = word2  # Store the second word in the second dictionary\n",
    "            break  # Stop once a two-word entry is found and split\n",
    "            \n",
    "    return first_dict, second_dict  # Return both dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e29b2d1-cc46-4528-9510-3f2408d2e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_two_terms(search_term):\n",
    "    two_terms = 'FALSE'\n",
    "    for key, value in search_term.items():\n",
    "        if isinstance(value, str) and len(value.split()) == 2: \n",
    "            #print(\"!two terms found\")\n",
    "            two_terms= 'TRUE'\n",
    "    return two_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba9676-4ff7-46a1-bd62-6ed93d82ffb6",
   "metadata": {},
   "source": [
    "### function to iterate through search terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c2e05c5-6544-4899-916b-ccf97838f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each row in the search terms CSV\n",
    "def search_term_iterate(search_terms_df, df, file_result):\n",
    "    for _, row in search_terms_df.iterrows():\n",
    "        # Process columns 'Token', 'Tag', and 'Lemma'\n",
    "        search_term = {\n",
    "        'Token': row['Token'],\n",
    "        'Tag': row['Tag'],\n",
    "        'Lemma': row['Lemma']\n",
    "        }\n",
    "        #print(search_terms_df)\n",
    "        #print(df)\n",
    "        #Here is defined what the column of the search result will look like\n",
    "        term_column_name = \".\".join(str(value) if pd.notna(value) else \"\" for value in search_term.values())\n",
    "        #print(search_term)\n",
    "        two_terms= check_two_terms(search_term)\n",
    "        #check for two word terms:\n",
    "        if two_terms=='FALSE':\n",
    "            #print(search_file_ci(search_term,df))\n",
    "            file_result[term_column_name] = len(search_file_ci(search_term,df))\n",
    "\n",
    " \n",
    "        if two_terms=='TRUE':\n",
    "            # Perform the split and assign to separate variables\n",
    "            single_term, following_term = split_two_word_entries(search_term)\n",
    "            indices_single_term = search_file_ci(single_term,df)\n",
    "            indices_following_term = search_file_ci(following_term,df)\n",
    "            # Keep only the indices that are present in both sets\n",
    "            #print(\"indices_single_term: \",indices_single_term)\n",
    "            #print(\"indices_following_term: \",indices_following_term)\n",
    "            common_indices = indices_single_term.intersection(indices_following_term-1)\n",
    "            # Output the result\n",
    "            #print(\"Two term indices:\", common_indices)\n",
    "            file_result[term_column_name] = len(common_indices)\n",
    "    return file_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901a35a-1745-4760-9458-05c0dbf0799a",
   "metadata": {},
   "source": [
    "## Main loop\n",
    "Here we iterate through folders, and text files to count the frequency of the search terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0329817a-adae-4e1d-b967-3600e0df336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: en_CNN\n",
      "Processing file 306/306 (100.00%) - ‘you decided to still drop a bomb’ wolf presses idf spokesman on israeli airstrike on refugee camp_output.json in folder: en_CNN\n",
      "Finished processing all files in folder: en_CNN, results saved to frequency_data_new\\en_CNN_frequency.csv\n",
      "\n",
      "Processing complete for all folders.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of files to be processed\n",
    "total_files = sum([len([name for name in os.listdir(folder) if name.endswith(\".json\")]) for folder in folder_paths])\n",
    "processed_files = 0\n",
    "# Loop through all folder paths\n",
    "for folder_path in folder_paths:\n",
    "    # Extract the middle section of the folder path\n",
    "    middle_folder_name = folder_path.split('/')[1]  # Adjust based on your folder structure\n",
    "    print(f\"Processing folder: {middle_folder_name}\")\n",
    "    \n",
    "    # Create a list to store data for the current folder\n",
    "    folder_results = []\n",
    "    \n",
    "    # Loop through all files in the current folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file is a .txt file\n",
    "        if file_name.endswith(\".json\"):\n",
    "            processed_files += 1\n",
    "            progress_percentage = (processed_files / total_files) * 100\n",
    "            print(f\"\\rProcessing file {processed_files}/{total_files} ({progress_percentage:.2f}%) - {file_name} in folder: {middle_folder_name}\", end='', flush=True)\n",
    "            \n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Open the JSON file and load the data with utf-8 encoding\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            # Accessing various parts of the JSON data\n",
    "            video_id = data['video_id']\n",
    "            publish_date = data['publish_date']\n",
    "            video_title = data['video_title']\n",
    "              \n",
    "            # Access the treetagger_output\n",
    "            treetagger_output = data['treetagger_output']\n",
    "              \n",
    "            # Convert the treetagger_output list of dictionaries to a DataFrame\n",
    "            content_df = pd.DataFrame(treetagger_output)\n",
    "            try:\n",
    "                total_word_count = content_df[\"Token\"].fillna('').str.split().str.len().sum()\n",
    "                total_word_count = int(total_word_count)  # Ensure it's an integer\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError: {e}. The column 'Token' does not exist in the DataFrame for File {file_path}\")\n",
    "                continue  # Skip this iteration and move to the next\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {e}\")\n",
    "                continue  # Skip this iteration for any other unexpected exceptions\n",
    "\n",
    "            # Initialize a dictionary to store term counts for this file and iterate through the search terms\n",
    "            file_result = {\n",
    "                \"file_name\": file_name,\n",
    "                \"video_id\": video_id,\n",
    "                \"publish_date\": publish_date,\n",
    "                \"total_word_count\": total_word_count\n",
    "            }\n",
    "            file_result = search_term_iterate(search_terms_df, content_df, file_result)\n",
    "            #print(file_result)\n",
    "            # Add the result dictionary to the list for this folder\n",
    "            folder_results.append(file_result)\n",
    "\n",
    "    # Convert the folder results into a DataFrame\n",
    "    folder_df = pd.DataFrame(folder_results)\n",
    "\n",
    "    # Save to a CSV file in the 'frequency_data' directory using the middle folder name\n",
    "    output_csv_path = os.path.join(output_directory, f\"{middle_folder_name}_{file_name_addition}.csv\")\n",
    "    folder_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"\\nFinished processing all files in folder: {middle_folder_name}, results saved to {output_csv_path}\")\n",
    "\n",
    "print(\"\\nProcessing complete for all folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66ebd2-edb3-427b-b008-9f7878e321e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
