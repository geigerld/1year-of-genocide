<video_id>DpHUG-brz04</video_id>
<video_title>Israel’s AI-powered 'mass assassination factory' | UpFront</video_title>
<thumbnail_path>data/en_AJ\Israel’s AI-powered 'mass assassination factory'  UpFront.jpg</thumbnail_path>
<publish_date>20231222</publish_date>

<doc>
a mass assassination Factory that was the headline on an investigation by 972 magazine and local call into the Israeli military's artificial intelligence-based targeting system called habps which means the gospel in English according to the investigation the system uses AI to produce targeting recommendations at a faster Pace than a team of humans working alone would ever be able to do joining me to discuss this is Laura Nolan she's a software engineer and a volunteer with stop Killer Robots Coalition Laura thanks so much for joining us on upfront good to see you again how does hop sort of work uh what kind of information would go into an AI based military targeting system like this um well unfortunately I what I can say is we don't know a huge amount for sure and that's very typical for these kinds of military decision support or automation systems I mean there's just a lot of secrecy that's involved um I suspect that they're probably using several different kind kinds of inputs so we know that there's an awful lot of surveillance that happens in Gaza uh for example uh almost certainly there will be Communications metadata and intercepts um there will be very possibly human intelligence and also from CCTV and uh facial recognition and any other sort of visual imagery that they may have which might include drone and satellite imagery so potentially there's an awful lot of information going into that system from all sorts of sources sources when I hear about this stuff for me as a lay person uh this sounds overwhelming this sounds like the most Cutting Edge technology possible but the thing I was wondering is are there even more sophisticated uh weaponry and and weapon systems than that that are on the horizon than just Habs um I don't think we know um so for example what I would say is Habs is not necessarily a weapon system in itself um I think the um Mass assassination Target Factory was a pretty opposite term so what it's doing is it's it's trying to identify targets which will then be struck with some other kind of weapon it's very much um a decision support system um but uh that doesn't say that doesn't mean to say there's not a lot of concerns around it um so one of the big talking points around this system is that it the scale of it it's um it's producing hundreds of targets per day potentially and we know that um at least 15 15,000 targets have been struck so far in the conflict um so that's um you know in the several hundreds per day um human beings simp do not have the ability to go and to um check in detail every one of those um decisions that the system is making and therefore accountability is very much being handed over to this system which um as you say we know very little about we don't know how it's been tested we don't know how it's been validated we don't know what criteria it's using to come up with these targets whatsoever although from the 972 investigation the system seems to be geared towards um prioritizing targets that are doing excess damage to civilian infrastructure and indeed potentially killing civilians and of course that's contrary to the laws of war which state that when you make a strike you have to be doing it for primarily military Advantage not to um put pressure on a population the investigation uh cites a Former Intelligence officer uh saying a human will review the targeting recommendation before an attack uh but this person also says the reviewer doesn't need to spend a lot of time on it uh are there enough human safeguards put into uh the decision-making structure here when it comes to an AI system like this it's a great question and I think one of the big questions when you have to assess the risk around anyi system is to what extent um are there safeguards around it in terms of um humans in the loop and in terms of um the overall riskiness of what's being done as a result of these decisions um I read the same article you did the same investigation and it absolutely points to um insufficient human time and effort spent in reviewing these targets um and targeting targeting particularly these kinds of dual use targets so people and um civilian buildings is extremely hard and time intensive so no I don't think they are from what I see I don't think they are properly reviewing these targets and they're very much leaving it in the hands of the automation system uh in a statement the Israeli Army claims that Habs helps in quote causing great damage to the enemy and minimal damage to non combatants now the death toll in Gaza is now over 18,000 many have made the case that evces and Technology are designed to make Warfare more precise that if it's more precise it'll be safer uh are you buying that argument no absolutely I'm not buying that argument and this is something that you hear all the time particularly from the the more high-tech militaries um typically when militaries think that they have these more these accurate these precise weapons um that they that they feel able to use them in builtup areas in urban areas in places with a lot of civilians and civilian infrastructure around and unfortunately you know no matter how well you hit the particular Point you're trying to hit and no matter how much um what you're striking as a valid military Target when you're using high explosive weapons in an urban area with civilians around you are going to do a lot of damage so one of the side effects of more Precision can actually convert can perversely be more civilian damage and civilian harm wow so so what do you do if the more Precision that you sort of design for uh the more potential damage you have to civilian infrastructure to people in these areas that you described then what's the solution here from a technological perspective from a design perspective there isn't no techn technological solution here political solution is what's required if you want to safeguard civilians in this situation there is no technological technological solution for safeguarding civilians in war um more Precision is not the answer to it uh you quit Google in protest over project Maven a contract for the US defense department that would use AI to analyze uh Drone footage uh Maven doesn't sound much different than hsur has what you were afraid of just become our reality absolutely yeah this is exactly what I was thinking about five or six years ago when I was at Google and um when when Maven was coming down the line um you know once you have all of this data coming out of a system people will want to Crunch that data draw conclusions from it and act based on those conclusions no matter how strained your your chain of causality and indeed the legal basis for these decisions that you're making is Maven was a surveillance machine um and it it was analyzing people's movements people's social networks um people whole timelines of people's behavior and their pattern of lives and this is most certainly what Habs is doing as well very very similar Technologies very very similar um problems with them the European Union has just agreed on rules uh to govern the use of AI in the European market uh but the artificial intelligence act uh as it's called doesn't apply to defense applications or military applications uh did the EU miss an opportunity to properly regulate military AI how do you do that how do you regulate Ai and warfare it's a very difficult question um yes um the AI Act is not focused on Military applications whatsoever and you know in reality um an EU Act is probably not the right venue to be thinking about how the military applications of AI technology the right venue was probably the United Nations or some kind of treaty process to come up with legally binding rules and instruments to regulate these things among all nations um how do you regulate it um well my belief is that um we should not be using technology to generate targeting information particularly when we come to the most sensitive and risky targets which are people and dual use um targets the reason for that is um it's difficult problem to decide whether a individual person or a dual use building or dual use vehicle is a valid military Target or not um we do not have technology that is capable of even beginning to decide whether somebody is U involved in combat or is a member of an armed group um these matters are not something that you can tell by a photo these matters are not something that you can um you know tell with uh with with by by deciding if somebody has been swapping the SIM card in their phone um these things uh are difficult and sensitive decisions and um and they require a lot of data and a lot of human Jud judgement human judgment let me let me I just before we go just that word human judgment keeps coming back to me all this stuff comes down to human decision making human judgments whether we look at data or not how we look at data all these things are human judgments and yet much of the public conversation is about the Killer Robots it's about the AI technology should we be focusing more on humans and blaming humans rather than technology um we should be focusing on holding humans accountable for for what is done in Warfare and um we have there are a lot of gaps and a lot of gray areas in international humanitarian law the laws of war but what is not a gray area is that when you make a military strike your objective has to be primarily to to to obtain this military effect sometimes it is permissible under the laws of war for to there to be collateral damage but in this case we have a very clear-cut uh legal problem which is Israel is making strikes with the primary aim of harming civilian infrastructure and using the fact that there is um that they believe that there is a military Target present as a fake Lea for that and I think that's something that people should be held accountable for and absolutely that accountability cannot be transferred to any software system or any computer system L Nolan I want to thank you for joining us on up front all right that is our show up front we'll be back next week
</doc>