{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0329817a-adae-4e1d-b967-3600e0df336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: en_CNN\n",
      "Processing file 306/306 (100.00%) - ‘you decided to still drop a bomb’ wolf presses idf spokesman on israeli airstrike on refugee camp_treetagger_output.txt in folder: en_CNN\n",
      "Finished processing all files in folder: en_CNN, results saved to frequency_data\\en_CNN_frequency.csv\n",
      "\n",
      "Processing complete for all folders.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk import bigrams, FreqDist\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Path to the folder containing the .txt files\n",
    "folder_path = \"data/en_DW/treetagger_output/\"\n",
    "output_csv_path = \"data/en_DW/keyword_frequencies_collocations.csv\"\n",
    "\n",
    "# Define custom stop words\n",
    "custom_stop_words = ['uh', 'um']\n",
    "stop_words = custom_stop_words + list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Function to extract metadata (video_id and publish_date) from a DataFrame\n",
    "def extract_metadata(df):\n",
    "    video_id = None\n",
    "    publish_date = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if '<video_id>' in row.iloc[0]:\n",
    "            video_id = re.search(r'<video_id>(.*?)</video_id>', row.iloc[0]).group(1)\n",
    "        if '<publish_date>' in row.iloc[0]:\n",
    "            publish_date = re.search(r'<publish_date>(.*?)</publish_date>', row.iloc[0]).group(1)\n",
    "        if video_id and publish_date:\n",
    "            break\n",
    "\n",
    "    return video_id, publish_date\n",
    "\n",
    "# Function to extract text from a DataFrame\n",
    "def extract_text(df):\n",
    "    text = ' '.join(df.iloc[:, 0].astype(str).tolist())  # Join all rows from the first column\n",
    "    return text\n",
    "\n",
    "# Read and process all text data\n",
    "texts = []\n",
    "metadata = []\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path, delimiter=\"\\t\", header=None)\n",
    "\n",
    "        # Extract text and metadata\n",
    "        text = extract_text(df)\n",
    "        video_id, publish_date = extract_metadata(df)\n",
    "\n",
    "        texts.append(text)\n",
    "        metadata.append({\"video_id\": video_id, \"publish_date\": publish_date})\n",
    "\n",
    "# Preprocess the text data with custom stop words\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=1000)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Calculate TF-IDF scores\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = X.sum(axis=0).A1\n",
    "tfidf_dict = dict(zip(feature_names, tfidf_scores))\n",
    "\n",
    "# Sort terms by their TF-IDF score\n",
    "sorted_tfidf = sorted(tfidf_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get top keywords\n",
    "top_keywords = [term for term, score in sorted_tfidf[:10]]  # Adjust the number of top keywords as needed\n",
    "\n",
    "# Dictionaries to store frequency and collocation results\n",
    "frequency_results = []\n",
    "total_frequencies = {keyword: 0 for keyword in top_keywords}\n",
    "total_word_count = 0\n",
    "collocation_freq = FreqDist()\n",
    "\n",
    "# Function to calculate collocations with a context window\n",
    "def calculate_collocations_with_window(text, top_keywords, window=3):\n",
    "    tokens = nltk.word_tokenize(text.lower())  # Tokenize the text\n",
    "    keyword_collocations = FreqDist()  # Store collocations involving top keywords\n",
    "\n",
    "    # Loop through each token to create a window of surrounding words\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in top_keywords:\n",
    "            # Define the start and end of the window\n",
    "            start = max(i - window, 0)\n",
    "            end = min(i + window + 1, len(tokens))\n",
    "            context = tokens[start:end]  # Get the context window\n",
    "\n",
    "            # Generate all bigrams from the context\n",
    "            context_bigrams = list(bigrams(context))\n",
    "            for bigram in context_bigrams:\n",
    "                keyword_collocations[bigram] += 1  # Increment frequency of the bigram\n",
    "\n",
    "    return keyword_collocations\n",
    "\n",
    "def sketch_logdice(bigram, collocation_freq, keyword_freq):\n",
    "    word1, word2 = bigram\n",
    "\n",
    "    freq_bigram = collocation_freq[bigram]\n",
    "    freq_word1_context = sum(collocation_freq[(word1, other_word)] for other_word in collocation_freq if other_word != word1)\n",
    "    freq_word2_context = sum(collocation_freq[(other_word, word2)] for other_word in collocation_freq if other_word != word2)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if freq_word1_context + freq_word2_context == 0:\n",
    "        return float('-inf')  # Or another suitable value\n",
    "\n",
    "    # LogDice calculation with a base adjustment\n",
    "    logdice_score = 14 + np.log2((2 * freq_bigram) / (freq_word1_context + freq_word2_context))\n",
    "\n",
    "    return logdice_score\n",
    "\n",
    "# Loop through all files to count keyword frequencies and collocations\n",
    "for text, meta in zip(texts, metadata):\n",
    "    term_counts = {keyword: text.lower().count(keyword) for keyword in top_keywords}\n",
    "    word_count = len(text.split())\n",
    "    total_terms = sum(term_counts.values())\n",
    "\n",
    "    for keyword in top_keywords:\n",
    "        total_frequencies[keyword] += term_counts[keyword]\n",
    "    total_word_count += word_count\n",
    "\n",
    "    term_relative_frequencies = {keyword: (term_counts[keyword] / word_count) * 1_000_000 if word_count > 0 else 0 for keyword in top_keywords}\n",
    "\n",
    "    # Calculate collocations for each file using the updated function\n",
    "    file_collocations = calculate_collocations_with_window(text, top_keywords)\n",
    "    for bigram in file_collocations.keys():\n",
    "        collocation_freq[bigram] += file_collocations[bigram]\n",
    "\n",
    "    result = {\n",
    "        \"video_id\": meta[\"video_id\"],\n",
    "        \"publish_date\": meta[\"publish_date\"],\n",
    "        \"total_term_count\": total_terms,\n",
    "        \"total_word_count\": word_count\n",
    "    }\n",
    "    result.update(term_counts)\n",
    "    result.update({f\"{keyword}_relative\": term_relative_frequencies[keyword] for keyword in top_keywords})\n",
    "\n",
    "    frequency_results.append(result)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_results = pd.DataFrame(frequency_results)\n",
    "\n",
    "# Calculate LogDice for each collocation\n",
    "logdice_scores = {bigram: sketch_logdice(bigram, collocation_freq, total_frequencies) for bigram in collocation_freq.keys()}\n",
    "\n",
    "# Sort collocations by LogDice score\n",
    "sorted_collocations = sorted(logdice_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print out the top collocations based on LogDice\n",
    "print(\"Top Collocations by LogDice Score:\")\n",
    "for bigram, score in sorted_collocations[:20]:  # Adjust the number of top collocations as needed\n",
    "    print(f\"  Collocation: {bigram}, LogDice Score: {score:.4f}\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_results.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Calculate and print total frequencies and relative frequencies across all files\n",
    "print(\"Total Frequencies Across All Files:\")\n",
    "total_keyword_count = sum(total_frequencies.values())\n",
    "for keyword, count in total_frequencies.items():\n",
    "    print(f\"  {keyword} Total Count: {count}\")\n",
    "    print(f\"  {keyword} Relative Frequency: {(count / total_word_count) * 1_000_000:.2f} instances per million\")\n",
    "print(f\"  Total Word Count Across All Files: {total_word_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e442b",
   "metadata": {},
   "source": [
    "[Relationship Extraction & Network Analysis with Spacy & NetworkX](https://youtu.be/fAHkJ_Dhr50?si=8fDlto50iqekKnI-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b1eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
